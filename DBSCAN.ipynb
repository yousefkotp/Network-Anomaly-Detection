{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN Algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(path):\n",
    "    data = pd.read_csv(path)\n",
    "    x = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "    return x, y\n",
    "\n",
    "def splitData(x, y, train_size):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=train_size, random_state=0, stratify=y)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def preprocessData(x):\n",
    "    \n",
    "    # Encoding the categorical data\n",
    "    ct = ColumnTransformer( [('one_hot_encoder', OneHotEncoder(), [1, 2, 3])], remainder='passthrough' )\n",
    "    ct = ct.fit(x)\n",
    "    x = pd.DataFrame(ct.transform(x))\n",
    "\n",
    "    # Scaling the data since the features are in different scales\n",
    "    scaler = StandardScaler()\n",
    "    scaler = scaler.fit(x)\n",
    "    x = pd.DataFrame(scaler.transform(x))\n",
    "\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan(data, min_samples, eps):\n",
    "\n",
    "    # Initializing labels to -1\n",
    "    labels = np.full(data.shape[0], -1, dtype=int)\n",
    "\n",
    "    # Initializing visited set\n",
    "    visited = set()\n",
    "\n",
    "    # Initializing core_dict to store core points and map to their neighborhoods\n",
    "    core_dict = {}\n",
    "\n",
    "    # Computing the distance matrix\n",
    "    distanceMatrix = pairwise_distances(data, metric='euclidean')\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "\n",
    "        # Skipping if already visited\n",
    "        if i in visited:\n",
    "            continue\n",
    "        \n",
    "        # Visiting the point\n",
    "        visited.add(i)\n",
    "\n",
    "        # Finding neighbors\n",
    "        neighbors = [j for j in range(data.shape[0]) if distanceMatrix[i][j] <= eps]\n",
    "\n",
    "        if len(neighbors) < min_samples: # If not a core point\n",
    "            labels[i] = -1\n",
    "        else: # If a core point\n",
    "\n",
    "            # Assigning label to core point\n",
    "            labels[i] = i \n",
    "\n",
    "            # Adding core point to core_dict\n",
    "            core_dict[i] = neighbors\n",
    "            \n",
    "            for j in neighbors:\n",
    "                # Skipping if already visited\n",
    "                if j in visited:\n",
    "                    continue\n",
    "\n",
    "                # Finding neighbors of neighbors\n",
    "                neighbors2 = [k for k in range(data.shape[0]) if distanceMatrix[j][k] <= eps]\n",
    "\n",
    "                # Visiting the point\n",
    "                visited.add(j)\n",
    "\n",
    "                # Assigning label of the neighbot to the same one as core point\n",
    "                labels[j] = i\n",
    "\n",
    "                # Adding neighbors of neighbors to core_dict which will be assigned the same label as core point later in the last loop\n",
    "                if len(neighbors2) >= min_samples:\n",
    "                    core_dict[j] = neighbors2\n",
    "                \n",
    "    # Assigning labels to non-core points based on their core point\n",
    "    for label,neighborhood in core_dict.items():\n",
    "        for neighbor in neighborhood:\n",
    "            if labels[neighbor] == -1:\n",
    "                labels[neighbor] = label\n",
    "    \n",
    "    # Convert labels to integers\n",
    "    labels = labels.astype(int)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "x, y = loadData('archive/kddcup.data.corrected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the data\n",
    "x = preprocessData(x)\n",
    "print(\"The shape of the training data is: \", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Data into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and test\n",
    "x_train, x_test, y_train, y_test = splitData(x, y, train_size=0.00025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the x_train and y_train as numpy arrays\n",
    "np.save('hierarchicalClustering-Preprocessed/x_train.npy', x_train)\n",
    "np.save('hierarchicalClustering-Preprocessed/y_train.npy', y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the preprocessed data as numpy arrays\n",
    "x_train = np.load('Clustering_data-preprocessed/x_train.npy', allow_pickle=True)\n",
    "y_train = np.load('Clustering_data-preprocessed/y_train.npy', allow_pickle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of noise points:  179\n",
      "Number of clusters:  23\n"
     ]
    }
   ],
   "source": [
    "# Testing the DBSCAN algorithm\n",
    "labelsInNumbers = dbscan(x_train, 5, 0.5)\n",
    "\n",
    "# Print number of noise points (label = -1)\n",
    "print('Number of noise points: ', np.count_nonzero(labelsInNumbers == -1))\n",
    "\n",
    "# Create array of strings to store the final labels\n",
    "labelsInString = np.empty(labelsInNumbers.shape[0], dtype=object)\n",
    "\n",
    "# The number of clusters will be equal to the number of unique labels\n",
    "clusters = np.unique(labelsInNumbers)\n",
    "print('Number of clusters: ', clusters.shape[0])\n",
    "\n",
    "for i in range(clusters.shape[0]):\n",
    "\n",
    "    # Finding the indices of the points in the cluster\n",
    "    labels = np.where(labelsInNumbers == clusters[i])\n",
    "    \n",
    "    # Creating a dictionary to count the number of each label in the cluster\n",
    "    counterLabels = {}\n",
    "    for label in y_train[labels]:\n",
    "        counterLabels[label] = counterLabels.get(label, 0) + 1\n",
    "\n",
    "    # Finding the most common label\n",
    "    maxLabel = max(counterLabels, key=counterLabels.get)\n",
    "\n",
    "    # Assigning the most common label to all the points in the cluster\n",
    "    labelsInString[labels] = maxLabel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro: \n",
      "Precision:  0.4114906832298137\n",
      "Recall:  0.41717389851718206\n",
      "F1 score:  0.41356096108100876\n",
      "Accuracy:  0.9730392156862745\n",
      "--------------------------------------------------\n",
      "Weighted: \n",
      "Precision:  0.9672758596192099\n",
      "Recall:  0.9730392156862745\n",
      "F1 score:  0.9690527951217186\n",
      "Accuracy:  0.9730392156862745\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    ipsweep.       0.00      0.00      0.00         3\n",
      "    neptune.       1.00      0.92      0.96       268\n",
      "       nmap.       0.00      0.00      0.00         1\n",
      "     normal.       0.88      1.00      0.94       243\n",
      "  portsweep.       0.00      0.00      0.00         3\n",
      "      satan.       0.00      0.00      0.00         4\n",
      "      smurf.       1.00      1.00      1.00       702\n",
      "\n",
      "    accuracy                           0.97      1224\n",
      "   macro avg       0.41      0.42      0.41      1224\n",
      "weighted avg       0.97      0.97      0.97      1224\n",
      "\n",
      "The entropy of cluster  0  is:  1.0063698192801032\n",
      "The entropy of cluster  1  is:  -0.0\n",
      "The entropy of cluster  2  is:  -0.0\n",
      "The entropy of cluster  3  is:  -0.0\n",
      "The entropy of cluster  4  is:  -0.0\n",
      "The entropy of cluster  5  is:  -0.0\n",
      "The entropy of cluster  6  is:  -0.0\n",
      "The entropy of cluster  7  is:  -0.0\n",
      "The entropy of cluster  8  is:  -0.0\n",
      "The entropy of cluster  9  is:  -0.0\n",
      "The entropy of cluster  10  is:  -0.0\n",
      "The entropy of cluster  11  is:  -0.0\n",
      "The entropy of cluster  12  is:  -0.0\n",
      "The entropy of cluster  13  is:  -0.0\n",
      "The entropy of cluster  14  is:  -0.0\n",
      "The entropy of cluster  15  is:  -0.0\n",
      "The entropy of cluster  16  is:  -0.0\n",
      "The entropy of cluster  17  is:  -0.0\n",
      "The entropy of cluster  18  is:  -0.0\n",
      "The entropy of cluster  19  is:  -0.0\n",
      "The entropy of cluster  20  is:  -0.0\n",
      "The entropy of cluster  21  is:  -0.0\n",
      "The entropy of cluster  22  is:  -0.0\n",
      "Average conditional entropy:  0.04375520953391753\n"
     ]
    }
   ],
   "source": [
    "y_pred = labelsInString\n",
    "\n",
    "# Evaluating the algorithm\n",
    "print(\"Macro: \")\n",
    "print(\"Precision: \", precision_score(y_train, y_pred, average='macro'))\n",
    "print(\"Recall: \", recall_score(y_train, y_pred, average='macro'))\n",
    "print(\"F1 score: \", f1_score(y_train, y_pred, average='macro'))\n",
    "print(\"Accuracy: \", accuracy_score(y_train, y_pred))\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Weighted: \")\n",
    "print(\"Precision: \", precision_score(y_train, y_pred, average='weighted'))\n",
    "print(\"Recall: \", recall_score(y_train, y_pred, average='weighted'))\n",
    "print(\"F1 score: \", f1_score(y_train, y_pred, average='weighted'))\n",
    "print(\"Accuracy: \", accuracy_score(y_train, y_pred))\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(classification_report(y_train, y_pred))\n",
    "\n",
    "# Now, let's measure the conditional entropy of the clusters to see if they are well separated\n",
    "totalEntropy = 0\n",
    "for i in range(clusters.shape[0]):\n",
    "    entropy = 0\n",
    "    # Getting the label indices of the points in the cluster\n",
    "    labels = np.where(labelsInNumbers == clusters[i])\n",
    "\n",
    "    # Getting the actual labels of the points in the cluster\n",
    "    labels = y_train[labels]\n",
    "\n",
    "    # Getting the counts of each label in each cluster\n",
    "    labels, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "    entropy = -np.sum(counts / np.sum(counts) * np.log2(counts / np.sum(counts)))\n",
    "\n",
    "    print(\"The entropy of cluster \", i, \" is: \", entropy)\n",
    "    totalEntropy += entropy\n",
    "\n",
    "# Dividing by the number of clusters to get the average conditional entropy\n",
    "totalEntropy /= clusters.shape[0]\n",
    "print(\"Average conditional entropy: \", totalEntropy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
